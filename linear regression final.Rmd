---
title: "Linear Regression Methods"
output: html_document
date: "2024-07-26"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r Libraries}

library(tidyverse)
library(dplyr)
library(gbm)
library(tree)
library(rpart)
library(caret)
library(MASS)
library(randomForest)
library(rpart.plot)
library(ISLR)
library(leaps)
library(glmnet)
library(ggplot2)

```

Subset Regression (reg)
```{r}

# Check for missing values
sum(is.na(regular_data))

# Fit regsubsets model on the training data
regfit <- regsubsets(popularity ~ ., data = regular_train, nvmax = 30, really.big = TRUE) # nvmax adjusts number of variables # really.big = true allows for a big search

# Create a function to calculate RMSE
rmse <- function(actual, predicted) {
  sqrt(mean((actual - predicted)^2)) }

# Evaluate models on the test data
# testX views predictors
# -1 excludes the intercept column
testX <- model.matrix(popularity ~ ., regular_test) # predictor matrix
# testY compares response variable actual values vs predicted
testY <- regular_test$popularity # response vector

# Get the actual number of predictors used in the regfit object
num_predictors <- length(summary(regfit)$outmat[1,])

# Citation: asked ChatGPT how to store values
# Create a vector to store RMSE values for each model
# rep() repeats vector x 
rmse_values <- rep(NA, num_predictors) 

# Loop over each model size from 1 to number of predictors
for (i in 1:num_predictors) {
  # Get the coefficients of the i-th model from the regsubsets result
  # 'id = i' specifies which model to extract, starting from 1 until id = nvmax
  coef_i <- coef(regfit, id = i)
  
  # Predict the response variable using the i-th model coefficients
  # We select only the columns corresponding to the predictors included in the i-th model
    # '%*%' is the matrix multiplication operator in R, calculates predicted values.
  pred_i <- testX[, names(coef_i)] %*% coef_i
  
  # Calculate the RMSE for the i-th model's predictions on the test data
  rmse_values[i] <- rmse(testY, pred_i)
  
  # Print (concatenate) the details of the i-nth model
  cat("\nModel with", i, "predictors\n")
  cat("Predictors:", names(coef_i), "\n")
  cat("RMSE:", rmse_values[i], "\n")
}

# Identify the model with the lowest RMSE
best_model_index <- which.min(rmse_values)
best_model_rmse <- rmse_values[best_model_index]

# Output the best model index and its RMSE
print(paste("Best model index:", best_model_index))
print(paste("Best model RMSE:", best_model_rmse))

# Get the coefficients of the best model
best_model_coef <- coef(regfit, id = best_model_index)
print("Coefficients of the best model:")
print(best_model_coef)

# Plotting
rmse_df <- data.frame(
  Model = 1:num_predictors,
  RMSE = rmse_values
)

# Plot the RMSE values
ggplot(rmse_df, aes(x = Model, y = RMSE)) +
  geom_line() +
  geom_point() +
  labs(title = "RMSE vs. Number of Predictors",
       x = "Number of Predictors",
       y = "RMSE") +
  theme_minimal()


```
Shrinkage lasso and ridge (reg)
```{r}
# Prepare the data
trainX <- model.matrix(popularity ~ ., regular_train)[, -1] # Predictor matrix for training
trainY <- regular_train$popularity # Response vector for training
testX <- model.matrix(popularity ~ ., regular_test)[, -1] # Predictor matrix for testing
testY <- regular_test$popularity # Response vector for testing

# Citation: asked ChatGPT how to determine the minimum RSE with lasso and ridge
# Fit a Lasso regression model
lasso_fit <- cv.glmnet(trainX, trainY, alpha = 1) # alpha = 1 for Lasso
lasso_pred <- predict(lasso_fit, s = "lambda.min", newx = testX) # lambda.min gives lambda with the least error
lasso_rmse <- sqrt(mean((testY - lasso_pred)^2))
lasso_coef <- coef(lasso_fit, s = "lambda.min")
print(lasso_coef)
# Lasso sets some variable coefficients to 0 (effectively removing them)

# Fit a Ridge regression model
ridge_fit <- cv.glmnet(trainX, trainY, alpha = 0) # alpha = 0 for Ridge
ridge_pred <- predict(ridge_fit, s = "lambda.min", newx = testX)
ridge_rmse <- sqrt(mean((testY - ridge_pred)^2))
ridge_coef <- coef(ridge_fit, s = "lambda.min")
print(ridge_coef)
# Ridge shrinks the coefficients but never removes the variables (zero)

# Print the RMSE for both models
print(paste("Lasso RMSE:", lasso_rmse))
print(paste("Ridge RMSE:", ridge_rmse))


# Lasso produces slightly better rmse
# Lasso tends to perform better when a small number of predictors have a strong effect on Y

# Citation: asked ChatGPT how to plot and scale rmse values by method
# Create a data frame for plotting
rmse_methods_df <- data.frame(
  Method = c("Subset Selection", "Lasso", "Ridge"),
  RMSE = c(best_model_rmse, lasso_rmse, ridge_rmse)
)

# Plot the RMSE values for different methods with a zoomed-in view
ggplot(rmse_methods_df, aes(x = Method, y = RMSE)) +
  geom_bar(stat = "identity") +
  labs(title = "RMSE Comparison of Different Methods",
       x = "Method",
       y = "RMSE") +
  theme_minimal() +
  scale_y_continuous(limits = c(0, max(rmse_methods_df$RMSE) * 1.1), labels = scales::number_format(accuracy = 0.001)) +
  geom_text(aes(label = sprintf("%.3f", RMSE)), vjust = -0.5, size = 3.5) +
  coord_cartesian(ylim = c(min(rmse_methods_df$RMSE) - 0.1, max(rmse_methods_df$RMSE) + 0.1)) # zooms in on y-axis

```