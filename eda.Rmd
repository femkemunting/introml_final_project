---
title: "eda"
output: html_document
date: "2024-07-19"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
#test
``` {Libraries}

library(tidyverse)
library(dplyr)
library(gbm)
library(tree)
library(rpart)
library(caret)
library(MASS)
library(randomForest)
library(rpart.plot)
library(math)

set.seed(1)
```

Our first step is to explore our data set. The Kaggle website for the data set had relatively complete variable descriptions, as well as 

``` {r EDA Part 1}
spotify = read.csv("spotify_dataset.csv")

```

Removing rows with time_signature = 0
```{r filtering}
filtered_spotify <- 
  spotify %>% 
  filter(
    time_signature %in% c(3, 4, 5, 6, 7))

attach(filtered_spotify)

# No missing values:

sum_isna <- sum(is.na(filtered_spotify))
sum_isna


# Count duplicates:

sum(duplicated(track_id))

# Remove duplicates that have the same track_name, artists and only keep the most popular song
filtered_spotify <-
  filtered_spotify %>%
  group_by(track_name, artists) %>% 
  filter(popularity == max(popularity))


 
#Test:  
filtered_spotify %>% filter(track_name == "Run Rudolph Run")



# View duplicate track_ids
duplicates_sorted <- filtered_spotify %>%
  group_by(track_id) %>%
  filter(n() > 1) %>%
  arrange(track_id)

 
# Some songs have the same track_id but are in different genres

filtered_spotify <-
  filtered_spotify %>% 
  group_by(track_id) %>% 
  sample_n(1)
  
duplicates_sorted_2 <- filtered_spotify %>%
  group_by(track_id) %>%
  filter(n() > 1) %>%
  arrange(track_id)
  
  
```

``` {r Plotting}
plot(danceability ~ tempo)
plot(danceability, popularity)



cor(tempo, danceability)


```

Notes:
* train_indices stores the indices for which rows will be part of the train set and which will be part of the test set. This split is different from the cross-validation splits; those will come from the 80% of training data we set up here. 
* $Resample1 is created by **createDataPartition()**
* kcv will be kept at 10 (could change it to 5, but 10 seems fine)
* I'm trying a boosting model first, following closely from Professor Murray's examples in class
* Should we use the 
``` {r Popularity Model Set-Up}

train_indices = createDataPartition(filtered_spotify$popularity,
                               p = 0.8)
spotify_train = filtered_spotify[train_indices$Resample1,]
spotify_test = filtered_spotify[-train_indices$Resample1,]

kcv = 10

cv_folds = createFolds(boston_train$popularity,
                       k = kcv)

fit_control <- trainControl(
  method = "cv",
  indexOut = cv_folds,
  selectionFunction="oneSE")

gbm_fit <- train(popularity ~ . - track_id - artists - album_name - genre - track_name - explicit,
                data = spotify_train,
                method = "gbm",
                trControl = fit_control,
                verbose = FALSE)

gbm_grid <-  expand.grid(interaction.depth = c(1, 2, 3, 5, 10), 
                        n.trees = c(150, 500, 1000), 
                        shrinkage = c(0.01, 0.1, 0.2),
                        n.minobsinnode = 10)


```