---
title: "eda"
output: html_document
date: "2024-07-19"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
#test
``` {r Libraries}

library(tidyverse)
library(dplyr)
library(gbm)
library(tree)
library(rpart)
library(caret)
library(MASS)
library(randomForest)
library(rpart.plot)
library(car)
library(ggfortify)

```

Our first step is to explore our data set. The Kaggle website for the data set had relatively complete variable descriptions, as well as 

``` {r EDA Part 1}
spotify = read.csv("spotify_dataset.csv")

```

Removing rows with time_signature = 0
```{r filtering}
filtered_spotify <- 
  spotify %>% 
  filter(
    time_signature %in% c(3, 4, 5, 6, 7))


attach(filtered_spotify)

# No missing values:

sum_isna <- sum(is.na(filtered_spotify))
sum_isna


# Count duplicates:

sum(duplicated(track_id))

# Remove duplicates that have the same track_name, artists and only keep the most popular song
filtered_spotify <-
  filtered_spotify %>%
  group_by(track_name, artists) %>% 
  filter(popularity == max(popularity))


 
#Test:  
filtered_spotify %>% filter(track_name == "Run Rudolph Run")



# View duplicate track_ids
duplicates_sorted <- filtered_spotify %>%
  group_by(track_id) %>%
  filter(n() > 1) %>%
  arrange(track_id)

 
# Some songs have the same track_id but are in different genres

filtered_spotify <-
  filtered_spotify %>% 
  group_by(track_id) %>% 
  sample_n(1)
  
duplicates_sorted_2 <- filtered_spotify %>%
  group_by(track_id) %>%
  filter(n() > 1) %>%
  arrange(track_id)
  
filtered_spotify <-
  filtered_spotify %>% 
  mutate(mode = factor(mode),
         key = factor(key),
         explicit = factor(explicit, levels = c("True", "False")),
         track_genre = factor(track_genre)
         )
summary(filtered_spotify)

model_data <- subset(filtered_spotify, select = -c(X, track_id, track_name, album_name, artists))

```

``` {r Plotting}
plot(danceability ~ tempo)
plot(danceability, popularity)

non_quant_variables = c("track_id", "artists", "album_name", "track_name", "explicit", "track_genre")

quant_variables <-
  filtered_spotify %>% 
  select_if(is.numeric) 


quant_variables <- quant_variables[,  2:16]

cor(quant_variables)

pairs(quant_variables)

```

Linear model to see relation between variables
```{r}
spotify_lm_dataset <- filtered_spotify %>% select(-X, -track_id, -artists, -album_name, -track_name, -track_genre, -explicit)
filtered_spotify_lm <- lm(popularity ~ . - X - track_id - artists - album_name - track_name, data = filtered_spotify)
summary(filtered_spotify_lm)
plot(spotify_lm_dataset$danceability, spotify_lm_dataset$popularity)
```

Cross-validation
```{r}
set.seed(18)

# Hold out 20% of the data as a final validation set
train_ix = createDataPartition(quant_variables$popularity,
                               p = 0.8)

spotify_train = quant_variables[train_ix$Resample1,]
spotify_test  = quant_variables[-train_ix$Resample1,]

###########################################################################
# Setup cross-validation
###########################################################################

# Define how we're going to estimate OOS error using cross-validation

# Number of folds
kcv = 10

# I'm manually making the folds here so we can look at them, and so
# they're the same when we evaluate each method below. If you omit
# the indexOut argument below caret with make the folds behind the scenes.

cv_folds = createFolds(spotify_train$popularity,
                               k = kcv)

# This function sets up how we're going to do our training: The method for
# estimating OOS error (CV) and associated settings (here the folds we created 
# above). I'm also going to request that our final fit is determined not by
# the minimum estimated OOS RMSE but using the one standard deviation (aka
# one standard error) rule instead by specifying selectionFunction="oneSE"

fit_control <- trainControl(
  method = "cv",
  indexOut = cv_folds,
  selectionFunction="oneSE")
```

Boosting
```{r}
###########################################################################
# Boosting
###########################################################################

# Boosting, optimizing over default grid for number of trees and depth
gbmfit <- train( popularity ~ ., data = spotify_train, 
                 method = "gbm", 
                 trControl = fit_control,
                 verbose = FALSE)

# Using a custom grid
gbm_grid <-  expand.grid(interaction.depth = c(1, 2, 3, 5, 10), 
                        n.trees = c(150, 500, 1000), 
                        shrinkage = c(0.01, 0.1, 0.2),
                        n.minobsinnode = 10)

gbmfit_2 <- train(popularity ~ ., data = spotify_train, 
                 method = "gbm", 
                 trControl = fit_control,
                 tuneGrid = gbm_grid,
                 verbose = FALSE)

print(gbmfit_2)
```

Linear regression
```{r}
options(scipen=999)
lm_model_data <- lm(popularity ~ ., data=model_data)
summary(lm_model_data)

# Check the reference category of track_genre
levels(model_data$track_genre)
# The reference category is german

# Check the reference category of key
levels(model_data$key)
# The reference category is 8

# Each variable (except some specific categories) are statistically significant
# Some categories have a large p-value
# Perform ANOVA test to compare models with and without categorical variables
lm_model_data_no_track_genre <- lm(popularity ~ . - track_genre, data=model_data)
summary(lm_model_data_no_track_genre)
# Adjusted R-squared is lower after removal, and higher RSE
anova(lm_model_data_no_track_genre, lm_model_data)
# F-test p-value is very small for track_genre --> track_genre is statistically significant

lm_model_data_no_key <- lm(popularity ~ . - key, data=model_data)
summary(lm_model_data_no_key)
# Adjusted R-squared is lower after removal, and higher RSE
anova(lm_model_data_no_key, lm_model_data)
# F-test p-value is very small for key --> key is statistically significant

# If a specific category is not statistically significant -> effect on pop is not distinguishable from reference category

# Backwards step-wise regression
step(lm_model_data, direction="backward")
# All initial variables were retained

autoplot(lm_model_data)
# view residuals vs leverage plot only
autoplot(lm_model_data, which=5)
# Outliers at 13795, 55236, 68307 - remove influential outliers
model_data2= model_data[-13795,]
model_data3= model_data2[-55236,]
model_data4 = model_data3[-68307,]
lm_model_data_adjusted <- lm(popularity ~ ., data=model_data4)
autoplot(lm_model_data_adjusted, which=5)
autoplot(lm_model_data_adjusted)
# linearity assumption satisfied (horizontal line on residuals vs fitted)
# normality assumption not satisfied (skewed towards the ends)
# independence assumption is satisfied because songs are independent from each other
# equal variance/homoscedasticity assumption is satisfied because there is relatively constant vertical thickness on the Residuals vs Fitted graph 

# check GVIF (since there are categorical var)
vif(lm_model_data_adjusted)
# no significant multicollinearity

# Final linear regression model used to predict popularity
summary(lm_model_data_adjusted)
confint(lm_model_data_adjusted)

# Coefficient Interpretation
# danceability - holding all other variables constant, popularity will increase by 4.48 for each increase in danceability
# energy - holding all other variables constant, popularity will decrease by -3.45 for each increase in energy
# track_genre uses german as it reference category
  # positive coefficients are expected to have higher popularity than german, negative coefficients are expected to have lower popularity than german

# Results
# R-squared
  # R-squared = 0.3738 -> 37.38% of the variation in popularity can be explained by the model
  # RSE = 15.75 -> for any prediction of carat we make using this model, we will on average be off by 0.02566 carats
# Statistical significance
  # model p-value < 2.2e-16 -> significant
# Practical significance
  # low r-squared -> not very practical :(

```


