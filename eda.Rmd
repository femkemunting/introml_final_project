---
title: "eda"
output: html_document
date: "2024-07-19"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
#test
``` {r Libraries}

library(tidyverse)
library(dplyr)
library(gbm)
library(tree)
library(rpart)
library(caret)
library(MASS)
library(randomForest)
library(rpart.plot)
library(car)
library(ggfortify)

```

Our first step is to explore our data set. The Kaggle website for the data set had relatively complete variable descriptions, as well as 

``` {r EDA Part 1}
spotify = read.csv("spotify_dataset.csv")

```

Removing rows with time_signature = 0
```{r filtering}
filtered_spotify <- 
  spotify %>% 
  filter(
    time_signature %in% c(3, 4, 5, 6, 7))


attach(filtered_spotify)

# No missing values:

sum_isna <- sum(is.na(filtered_spotify))
sum_isna


# Count duplicates:

sum(duplicated(track_id))

# Remove duplicates that have the same track_name, artists and only keep the most popular song
filtered_spotify <-
  filtered_spotify %>%
  group_by(track_name, artists) %>% 
  filter(popularity == max(popularity))


 
#Test:  
filtered_spotify %>% filter(track_name == "Run Rudolph Run")



# View duplicate track_ids
duplicates_sorted <- filtered_spotify %>%
  group_by(track_id) %>%
  filter(n() > 1) %>%
  arrange(track_id)

 
# Some songs have the same track_id but are in different genres

filtered_spotify <-
  filtered_spotify %>% 
  group_by(track_id) %>% 
  sample_n(1)
  
duplicates_sorted_2 <- filtered_spotify %>%
  group_by(track_id) %>%
  filter(n() > 1) %>%
  arrange(track_id)
  
filtered_spotify <-
  filtered_spotify %>% 
  mutate(mode = factor(mode),
         key = factor(key),
         explicit = factor(explicit, levels = c("True", "False")),
         track_genre = factor(track_genre)
         )
summary(filtered_spotify)

model_data <- subset(filtered_spotify, select = -c(X, track_id, track_name, album_name, artists))

```

``` {r Plotting}
plot(danceability ~ tempo)
plot(danceability, popularity)

non_quant_variables = c("track_id", "artists", "album_name", "track_name", "explicit", "track_genre")

quant_variables <-
  filtered_spotify %>% 
  select_if(is.numeric) 


quant_variables <- quant_variables[,  2:16]

cor(quant_variables)

pairs(quant_variables)

```

Linear model to see relation between variables
```{r}
spotify_lm_dataset <- filtered_spotify %>% select(-X, -track_id, -artists, -album_name, -track_name, -track_genre, -explicit)
filtered_spotify_lm <- lm(popularity ~ . - X - track_id - artists - album_name - track_name, data = filtered_spotify)
summary(filtered_spotify_lm)
plot(spotify_lm_dataset$danceability, spotify_lm_dataset$popularity)
```

Cross-validation
```{r}
set.seed(18)

# Hold out 20% of the data as a final validation set
train_ix = createDataPartition(quant_variables$popularity,
                               p = 0.8)

spotify_train = quant_variables[train_ix$Resample1,]
spotify_test  = quant_variables[-train_ix$Resample1,]

###########################################################################
# Setup cross-validation
###########################################################################

# Define how we're going to estimate OOS error using cross-validation

# Number of folds
kcv = 10

# I'm manually making the folds here so we can look at them, and so
# they're the same when we evaluate each method below. If you omit
# the indexOut argument below caret with make the folds behind the scenes.

cv_folds = createFolds(spotify_train$popularity,
                               k = kcv)

# This function sets up how we're going to do our training: The method for
# estimating OOS error (CV) and associated settings (here the folds we created 
# above). I'm also going to request that our final fit is determined not by
# the minimum estimated OOS RMSE but using the one standard deviation (aka
# one standard error) rule instead by specifying selectionFunction="oneSE"

fit_control <- trainControl(
  method = "cv",
  indexOut = cv_folds,
  selectionFunction="oneSE")
```

Boosting
```{r}
###########################################################################
# Boosting
###########################################################################

# Boosting, optimizing over default grid for number of trees and depth
gbmfit <- train( popularity ~ ., data = spotify_train, 
                 method = "gbm", 
                 trControl = fit_control,
                 verbose = FALSE)

# Using a custom grid
gbm_grid <-  expand.grid(interaction.depth = c(1, 2, 3, 5, 10), 
                        n.trees = c(150, 500, 1000), 
                        shrinkage = c(0.01, 0.1, 0.2),
                        n.minobsinnode = 10)

gbmfit_2 <- train(popularity ~ ., data = spotify_train, 
                 method = "gbm", 
                 trControl = fit_control,
                 tuneGrid = gbm_grid,
                 verbose = FALSE)

print(gbmfit_2)
```

Linear regression
```{r}
options(scipen=999)
lm_model_data <- lm(popularity ~ ., data=model_data)
summary(lm_model_data)

# Check the reference category of track_genre
levels(model_data$track_genre)
# The reference category is german

# Check the reference category of key
levels(model_data$key)
# The reference category is 8

# Each variable (except some specific categories) are statistically significant
# Some categories have a large p-value
# Perform ANOVA test to compare models with and without categorical variables
lm_model_data_no_track_genre <- lm(popularity ~ . - track_genre, data=model_data)
summary(lm_model_data_no_track_genre)
# Adjusted R-squared is lower after removal, and higher RSE
anova(lm_model_data_no_track_genre, lm_model_data)
# F-test p-value is very small for track_genre --> track_genre is statistically significant

lm_model_data_no_key <- lm(popularity ~ . - key, data=model_data)
summary(lm_model_data_no_key)
# Adjusted R-squared is lower after removal, and higher RSE
anova(lm_model_data_no_key, lm_model_data)
# F-test p-value is very small for key --> key is statistically significant

# If a specific category is not statistically significant -> effect on pop is not distinguishable from reference category

# Backwards step-wise regression
step(lm_model_data, direction="backward")
# All initial variables were retained

autoplot(lm_model_data)
# view residuals vs leverage plot only
autoplot(lm_model_data, which=5)
# Outliers at 13795, 55236, 68307 - remove influential outliers
model_data_cleaned <- model_data[-c(13795, 55236, 68307), ]
lm_model_data_cleaned <- lm(popularity ~ ., data=model_data_cleaned)
autoplot(lm_model_data_cleaned, which=5)
autoplot(lm_model_data_cleaned)
# linearity assumption satisfied (horizontal line on residuals vs fitted)
# normality assumption not satisfied (skewed towards the ends)
# independence assumption is satisfied because songs are independent from each other
# equal variance/homoscedasticity assumption is satisfied because there is relatively constant vertical thickness on the Residuals vs Fitted graph 

# check GVIF (since there are categorical var)
vif(lm_model_data_cleaned)
# no significant multicollinearity

# Final linear regression model used to predict popularity
summary(lm_model_data_cleaned)
confint(lm_model_data_cleaned)

# Coefficient Interpretation
# danceability - holding all other variables constant, popularity will increase by 4.48 for each increase in danceability
# energy - holding all other variables constant, popularity will decrease by -3.45 for each increase in energy
# track_genre uses german as it reference category
  # positive coefficients are expected to have higher popularity than german, negative coefficients are expected to have lower popularity than german

# Results
# R-squared
  # R-squared = 0.3738 -> 37.38% of the variation in popularity can be explained by the model
  # RSE = 15.75 -> for any prediction of carat we make using this model, we will on average be off by 0.02566 carats
# Statistical significance
  # model p-value < 2.2e-16 -> significant
# Practical significance
  # low r-squared -> not very practical :(

# Sum of Squares
anova(lm_model_data_cleaned)

# Prediction
predict(lm_model_data_cleaned, newdata = list(cut = "Premium", color = "D",
                                  clarity = "IF", depth = mean(diamonds4$depth),
                                  table = mean(diamonds4$table), price = mean(diamonds4$price), x = mean(diamonds4$x), y = mean(diamonds4$y)), interval="prediction")


predict(lm_model_data_cleaned, newdata = list(duration_ms = mean(model_data_cleaned$duration_ms), explicit = "True", danceability = mean(model_data_cleaned$danceability), energy = mean(model_data_cleaned$energy), key = "4", loudness = mean(model_data_cleaned$loudness), mode = "1", speechiness = mean(model_data_cleaned$speechiness), acousticness = mean(model_data_cleaned$acousticness), instrumentalness = mean(model_data_cleaned$instrumentalness), liveness = mean(model_data_cleaned$liveness), valence = mean(model_data_cleaned$valence), tempo = mean(model_data_cleaned$tempo), time_signature = 4, track_genre = "k-pop"), interval="prediction")
# fit is 60.86495,

```
Subset Regression 1
```{r}
# Show base model RMSE

# https://www.science.smith.edu/~jcrouser/SDS293/labs/lab8-r.html

model_data_cleaned2 <- model_data_cleaned
model_data_cleaned2_sample <- sample_n(model_data_cleaned2, 10000)

# removing track_genre
model_data_cleaned2_sample <- model_data_cleaned2_sample %>% select(-track_genre)

library(ISLR)
library(dplyr)
library(leaps)

# Check for missing values
model_data_cleaned2_sample %>%
  select(.) %>%
  is.na() %>%
  sum()

# The regsubsets() function (part of the leaps library) performs best subset selection by identifying the best model that contains a given number of predictors, where best is quantified using RSS. The syntax is the same as for lm(). The summary() command outputs the best set of variables for each model size.

regfit_full = regsubsets(popularity ~ . , data = model_data_cleaned2_sample)
summary(regfit_full)


```

Subset Regression 2
```{r}
library(ISLR)
library(dplyr)
library(leaps)
library(caret)

# Split the data into training and test sets
set.seed(18)

# Hold out 20% of the data as a final validation set
train_ix = createDataPartition(model_data_cleaned2_sample$popularity,
                               p = 0.8)

spotify_train = model_data_cleaned2_sample[train_ix$Resample1,]
spotify_test  = model_data_cleaned2_sample[-train_ix$Resample1,]

# Fit regsubsets model on the training data
regfit <- regsubsets(popularity ~ ., data = spotify_train, nvmax = 30, really.big = TRUE) # nvmax adjusts number of variables # really.big = true allows for a big search

# Create a function to calculate RMSE
rmse <- function(actual, predicted) {
  sqrt(mean((actual - predicted)^2)) }

# Evaluate models on the test data
# testX views predictors
# -1 excludes the intercept column
testX <- model.matrix(popularity ~ ., spotify_test) # predictor matrix
# testY compares response variable actual values vs predicted
testY <- spotify_test$popularity # response vector

# Get the actual number of predictors used in the regfit object
num_predictors <- length(summary(regfit)$outmat[1,])

# Create a vector to store RMSE values for each model
# rep() repeats vector x 
# 10 is used as the value because we specified nvmax = 10 earlier
rmse_values <- rep(NA, num_predictors) 

# Loop over each model size from 1 to 10 predictors
for (i in 1:num_predictors) {
  # Get the coefficients of the i-th model from the regsubsets result
  # 'id = i' specifies which model to extract, starting from 1 until id = nvmax
  coef_i <- coef(regfit, id = i)
  
  # Predict the response variable using the i-th model coefficients
  # We select only the columns corresponding to the predictors included in the i-th model
    # '%*%' is the matrix multiplication operator in R, calculates predicted values.
  pred_i <- testX[, names(coef_i)] %*% coef_i
  
  # Calculate the RMSE for the i-th model's predictions on the test data
  rmse_values[i] <- rmse(testY, pred_i)
  
  # Print (concatenate) the details of the i-nth model
  cat("\nModel with", i, "predictors\n")
  cat("Predictors:", names(coef_i), "\n")
  cat("RMSE:", rmse_values[i], "\n")
}

# Identify the model with the lowest RMSE
best_model_index <- which.min(rmse_values)
best_model_rmse <- rmse_values[best_model_index]

# Output the best model index and its RMSE
print(paste("Best model index:", best_model_index))
print(paste("Best model RMSE:", best_model_rmse))

# Get the coefficients of the best model
best_model_coef <- coef(regfit, id = best_model_index)
print("Coefficients of the best model:")
print(best_model_coef)

# Check the entire model
optimized_model <- lm(popularity ~ explicit + danceability + speechiness + instrumentalness + valence, data = model_data_cleaned2_sample)
summary(optimized_model)

# Plotting
rmse_df <- data.frame(
  Model = 1:num_predictors,
  RMSE = rmse_values
)

# Plot the RMSE values
ggplot(rmse_df, aes(x = Model, y = RMSE)) +
  geom_line() +
  geom_point() +
  labs(title = "RMSE vs. Number of Predictors",
       x = "Number of Predictors",
       y = "RMSE") +
  theme_minimal()


```
Subset Regression 3
```{r}
library(glmnet)
library(ggplot2)

# Prepare the data
trainX <- model.matrix(popularity ~ ., spotify_train)[, -1] # Predictor matrix for training
trainY <- spotify_train$popularity # Response vector for training
testX <- model.matrix(popularity ~ ., spotify_test)[, -1] # Predictor matrix for testing
testY <- spotify_test$popularity # Response vector for testing

# Fit a Lasso regression model
lasso_fit <- cv.glmnet(trainX, trainY, alpha = 1) # alpha = 1 for Lasso
lasso_pred <- predict(lasso_fit, s = "lambda.min", newx = testX)
lasso_rmse <- sqrt(mean((testY - lasso_pred)^2))
lasso_coef <- coef(lasso_fit, s = "lambda.min")
print(lasso_coef)
# Lasso sets some variable coefficients to 0 (effectively removing them)

# Fit a Ridge regression model
ridge_fit <- cv.glmnet(trainX, trainY, alpha = 0) # alpha = 0 for Ridge
ridge_pred <- predict(ridge_fit, s = "lambda.min", newx = testX)
ridge_rmse <- sqrt(mean((testY - ridge_pred)^2))
ridge_coef <- coef(ridge_fit, s = "lambda.min")
print(ridge_coef)
# Ridge shrinks the coefficients but never removes the variables (zero)

# Print the RMSE for both models
print(paste("Lasso RMSE:", lasso_rmse))
print(paste("Ridge RMSE:", ridge_rmse))


# Lasso produces slightly higher rmse
# Lasso tends to perform better when a small number of predictors have a strong effect on Y


# Create a data frame for plotting
rmse_methods_df <- data.frame(
  Method = c("Subset Selection", "Lasso", "Ridge"),
  RMSE = c(best_model_rmse, lasso_rmse, ridge_rmse)
)

# Plot the RMSE values for different methods with a zoomed-in view
ggplot(rmse_methods_df, aes(x = Method, y = RMSE)) +
  geom_bar(stat = "identity") +
  labs(title = "RMSE Comparison of Different Methods",
       x = "Method",
       y = "RMSE") +
  theme_minimal() +
  scale_y_continuous(limits = c(0, max(rmse_methods_df$RMSE) * 1.1), labels = scales::number_format(accuracy = 0.01)) +
  geom_text(aes(label = sprintf("%.2f", RMSE)), vjust = -0.5, size = 3.5) +
  coord_cartesian(ylim = c(min(rmse_methods_df$RMSE) - 0.1, max(rmse_methods_df$RMSE) + 0.1)) # zooms in on y-


```




