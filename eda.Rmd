---
title: "eda"
output: html_document
date: "2024-07-19"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
#test
``` {r Libraries}

packages_to_install <- c("tidyverse", "caret", "dplyr", "gbm", "tree", "rpart", "MASS", "randomForest", "rpart.plot")
installed <- rownames(installed.packages())
to_install <- packages_to_install[!packages_to_install %in% installed]
if (length(to_install) > 0) {
  install.packages(to_install)
} else {
  cat("All packages are already installed.\n")
}

library(tidyverse)
library(dplyr)
library(gbm)
library(tree)
library(rpart)
library(caret)
library(MASS)
library(randomForest)
library(rpart.plot)

```

Our first step is to explore our data set. The Kaggle website for the data set had relatively complete variable descriptions, as well as 
``` {r EDA Part 1}
spotify = read.csv("spotify_dataset.csv")

```


Removing rows with time_signature = 0
```{r filtering}
filtered_spotify <- 
  spotify %>% 
  filter(time_signature %in% c(3, 4, 5, 6, 7))

attach(filtered_spotify)

# No missing values:
sum_isna <- sum(is.na(filtered_spotify))
sum_isna


# Count duplicates:
sum(duplicated(track_id))


# Remove duplicates that have the same track_name, artists and only keep the most popular song
filtered_spotify <-
  filtered_spotify %>%
  group_by(track_name, artists) %>% 
  filter(popularity == max(popularity))


#Test:  
#filtered_spotify %>% filter(track_name == "Run Rudolph Run")


# View duplicate track_ids
duplicates_sorted <- filtered_spotify %>%
  group_by(track_id) %>%
  filter(n() > 1) %>%
  arrange(track_id)

 
# Some songs have the same track_id but are in different genres
filtered_spotify <-
  filtered_spotify %>% 
  group_by(track_id) %>% 
  sample_n(1)
  
duplicates_sorted_2 <- filtered_spotify %>%
  group_by(track_id) %>%
  filter(n() > 1) %>%
  arrange(track_id)

#factor categorical variables
filtered_spotify <-
  filtered_spotify %>% 
  mutate(mode = factor(mode),
         key = factor(key),
         explicit = factor(explicit, levels = c("True", "False")),
         track_genre = factor(ifelse(grepl("pop", track_genre, ignore.case = TRUE), 1, 0)))

#finding out how many categories in track genre because 53 is too many
#summary(filtered_spotify$track_genre)
#length(unique(filtered_spotify$track_genre))

```


``` {r Plotting and variable definitions}
#plot(danceability ~ tempo)
#plot(danceability, popularity)

non_quant_variables = c("track_id", "artists", "album_name", "track_name", "explicit", "track_genre")

quant_variables <-
  filtered_spotify %>% 
  select_if(is.numeric) 

quant_variables <- quant_variables[,  2:14]

#cor(quant_variables)
#pairs(quant_variables)

```


``` {r Popularity model setup}

#creates a new data frame containing only the specified columns 
#model_data <- filtered_spotify[ , c("popularity", "duration_ms", "danceability", "energy")]
model_data <- subset(filtered_spotify, select = -c(X, track_id, track_name, album_name, artists))
model_data_cleaned <- model_data[-c(13795, 55236, 68307), ]


#splits the data based on the popularity column to create training indices. In this case 80% of the data is reserved for training
train_indices = createDataPartition(model_data_cleaned$popularity,
                               p = 0.8)

#create training and testing data
spotify_train = model_data_cleaned[train_indices$Resample1,]
spotify_test = model_data_cleaned[-train_indices$Resample1,]


#creates folds for k cross validation
kcv = 10
cv_folds = createFolds(spotify_train$popularity,
                               k = kcv)

#sets up how training should be controlled, using cross validation
fit_control <- trainControl(
  method = "cv",
  indexOut = cv_folds,
  selectionFunction="oneSE")


#train a single tree model
tree_model <- train(popularity ~ ., data = spotify_train,
                    method = "rpart",
                    trControl = fit_control)


#plot the tree model
if(is(tree_model$finalModel, "rpart")) {
  rpart.plot(tree_model$finalModel, type = 3, extra = 101, under = TRUE, fallen.leaves = TRUE)
} else {
  print("The trained model is not a single tree model.")
}

###errorWarning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  : There were missing values in resampled performance measures.
#---This occurs because the factor(track genre) variable has more than 53 categories. https://stackoverflow.com/questions/26828901/warning-message-missing-values-in-resampled-performance-measures-in-caret-tra

```


``` {r caret_example.R}
###########################################################################
# Single tree
###########################################################################

#creates a dataframe with a series of values for complexity parameter (CP). 500 spaced points between .00001 and .03
#Outcome: This results in a wide range of cp values, allowing the model to explore various levels of tree complexity from very detailed (low cp values, allowing more splits) to more generalized models (high cp values, fewer splits).
rpart_grid = data.frame(cp = c(0, exp(seq(log(0.00001), log(0.03), length.out=500))))

#popularity is response and all other variables are predictors
single_tree_fit <- train( popularity ~ danceability + energy + loudness, data = spotify_train, 
                          method = "rpart", 
                          tuneGrid = rpart_grid,
                          trControl = fit_control)

#^^^^^^^^^^This block of code is used to perform hyperparameter tuning on the cp parameter of an rpart model. By doing so, it aims to find the optimal balance between model complexity and generalization capability, thus improving the model's predictive accuracy or minimizing overfitting, depending on the cross-validation results and the metric chosen for optimization. The outcome, single_tree_fit, will be an object containing the best-fitting model along with details about its performance across different cp values, which can be further analyzed or used for predictions.


#took almost 2 hours for all the variables
#danceability + energy + loudness variables only takes like 30 min

```



```{r caret_example.R}

# Extract the final fit
single_tree_fit$finalModel #gets the best tree model from the tuning process
rpart.plot(single_tree_fit$finalModel)
```



```{r caret_example.R}
# For this very special case, it's faster/more efficient to just use rpart
set.seed(1)
#popularity is response and all other variables are predictors
bigtree = rpart(popularity ~ danceability + energy + loudness, data = spotify_train,
                control = rpart.control(cp=0.0009, minsplit=5))
plotcp(bigtree) #plots a graph of the root mean square error (RMSE) against the complexity parameter for each potential tree size considered 

printcp(bigtree)

best_cp_ix = which.min(bigtree$cptable[,4]) # "Best"
bigtree$cptable[best_cp_ix,4]
```




```{From caret_example.R}
# one sd rule
tol = bigtree$cptable[best_cp_ix,4] + bigtree$cptable[best_cp_ix,5]
bigtree$cptable[bigtree$cptable[,4]<tol,][1,]
best_cp_onesd = bigtree$cptable[bigtree$cptable[,4]<tol,][1,1]
cvtree = prune(bigtree, cp=best_cp_onesd)


# Different looking trees -- mostly due to different CV folds -- 
# but very similar predictions!
plot(predict(cvtree), predict(single_tree_fit$finalModel))
abline(0,1)
# Jittering the predictions a little so they aren't on top of each other
plot(predict(cvtree)+runif(nrow(spotify_train), -0.5,  0.5), 
     predict(single_tree_fit$finalModel)+runif(nrow(spotify_train), -0.5,  0.5))
abline(0,1)

cor(predict(cvtree), predict(single_tree_fit$finalModel))

#########DO RMSE AND SHORTEN DEPTH

```
